{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['SPARK_HOME'] = \"D:\\program\\spark2.4.2\"\n",
    "os.environ['PYSPARK_PYTHON'] = \"D:\\program\\python3\\python3.exe\"\n",
    "os.environ['HADOOP_HOME'] = \"D:\\program\\hadoop-common-2.7.1-bin\"\n",
    "\n",
    "sys.path.append(\"D:\\program\\spark2.4.2\\bin\")\n",
    "sys.path.append(\"D:programspark2.4.2\\python\")\n",
    "sys.path.append(\"D:programspark2.4.2\\python\\pyspark\")\n",
    "sys.path.append(\"D:programspark2.4.2\\python\\lib\")\n",
    "sys.path.append(\"D:programspark2.4.2\\python\\lib\\pyspark.zip\")\n",
    "sys.path.append(\"D:programspark2.4.2\\python\\lib\\py4j-0.10.4-src.zip\")\n",
    "sys.path.append(\"D:\\program\\java8\")\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "\n",
    "sc = SparkContext(\"local\",\"testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local appName=testing>\n"
     ]
    }
   ],
   "source": [
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = sc.textFile(r\"E:\\dataset\\ml-20m\\ratings_noheader.csv\") #需要进行数据清洗，数据补全，不然后面会出错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1,2,3.5,1112486027'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3.5']\n"
     ]
    }
   ],
   "source": [
    "rates = user_data.map(lambda x: x.split(\",\")[0:3])\n",
    "print (rates.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating(user=1, product=2, rating=3.5)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.recommendation import Rating\n",
    "rates_data = rates.map(lambda x: Rating(int(x[0]),int(x[1]),float(x[2])))\n",
    "print (rates_data.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  pyspark.mllib.recommendation import ALS\n",
    "from pyspark.mllib.recommendation import MatrixFactorizationModel\n",
    "sc.setCheckpointDir('checkpoint/')\n",
    "ALS.checkpointInterval = 2\n",
    "model = ALS.train(ratings=rates_data, rank=20, iterations=5, lambda_=0.02) #这个步骤老是出现java.io.IOException: (null) entry in command string: null chmod 0644这种问题，需要添加合适的hadoop_home变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.297501876796062\n"
     ]
    }
   ],
   "source": [
    "print (model.predict(1,112)) #预测用户对摸个产品的评分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rating(user=56659, product=253, rating=7.021295905408631), Rating(user=123554, product=253, rating=6.805486120699479), Rating(user=71549, product=253, rating=6.642656716768418), Rating(user=103796, product=253, rating=6.5748299661609115), Rating(user=110749, product=253, rating=6.255862925709703), Rating(user=81103, product=253, rating=6.107264678510025), Rating(user=68658, product=253, rating=6.083039343870528), Rating(user=71934, product=253, rating=6.079418299685771), Rating(user=42482, product=253, rating=6.069459149662893), Rating(user=9886, product=253, rating=5.988796806115877)]\n"
     ]
    }
   ],
   "source": [
    "print (model.recommendUsers(253,10))#将商品推荐给10个最合适的用户，可以看到用户对这个商品的评价都超过5分了，满分才5分。。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rating(user=1, product=104390, rating=6.993582348535744), Rating(user=1, product=73529, rating=6.518953358851498), Rating(user=1, product=97300, rating=6.501910406515705), Rating(user=1, product=56779, rating=5.98893669429358), Rating(user=1, product=104583, rating=5.913042434398518), Rating(user=1, product=62206, rating=5.773360929900679), Rating(user=1, product=69464, rating=5.715772436339162), Rating(user=1, product=26033, rating=5.68207954792214), Rating(user=1, product=73194, rating=5.665705419399374), Rating(user=1, product=73139, rating=5.596993193337324)]\n"
     ]
    }
   ],
   "source": [
    "print(model.recommendProducts(1,10)) #推荐10个商品"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkContext' object has no attribute 'createDataFrame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-b63a45176180>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrates_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'SparkContext' object has no attribute 'createDataFrame'"
     ]
    }
   ],
   "source": [
    "#print(model.recommendProductsForUsers(5).collect())#给所有用户推荐5条产品，这个计算量很大，不要随便算\n",
    "\n",
    "spark = sc.getOrCreate()\n",
    "spark.createDataFrame(rates_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PipelinedRDD' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-2d1a5c2aa29d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrates_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'PipelinedRDD' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "print(rates_data.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
