{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('spark_sql_demo').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_data_body = spark.read.format('CSV').option('header','true').load(r\"E:\\dataset\\ml-20m\\ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(userId='1', movieId='2', rating='3.5', timestamp='1112486027')\n"
     ]
    }
   ],
   "source": [
    "print(rating_data_body.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[count(1): bigint]\n"
     ]
    }
   ],
   "source": [
    "rating_data_body.registerTempTable(\"user\") #必须要注册一下表名称，不然sql会找不到表\n",
    "rownum = spark.sql('select count(*) from user')\n",
    "print(rownum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(count(1)=20000263)\n"
     ]
    }
   ],
   "source": [
    "print(rownum.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(count(1),LongType,false)))\n"
     ]
    }
   ],
   "source": [
    "print(rownum.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[userId: string, movieId: string, rating: string, timestamp: string]\n"
     ]
    }
   ],
   "source": [
    "print(rating_data_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[userId: string, movieId: string, rating: float, timestamp: string]\n"
     ]
    }
   ],
   "source": [
    "rating_data_body = rating_data_body.withColumn(\"rating\", rating_data_body['rating'].cast('float'))\n",
    "print(rating_data_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[movieId: string, avg(rating): double]\n"
     ]
    }
   ],
   "source": [
    "avg_rate=rating_data_body.groupby(\"movieId\").avg(\"rating\") #求每部电影的平均评分，但是报rating\" is not a numeric column. \n",
    "#Aggregation function can only be applied on a numeric column\n",
    "print(avg_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(movieId='296', avg(rating)=4.174231169217055)\n"
     ]
    }
   ],
   "source": [
    "print(avg_rate.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[movieId: string, avg(rating): double]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.setCheckpointDir(\"./chkpt\")\n",
    "avg_rate.checkpoint()#由于计算量比较大，不存快照每次都要重新计算，所以调用检查点来坐一下缓存，但是报没有缓存目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|movieId|       avg(rating)|\n",
      "+-------+------------------+\n",
      "|    296| 4.174231169217055|\n",
      "|   1090| 3.919977226720648|\n",
      "|   3959| 3.699372603694667|\n",
      "|   2294| 3.303207714257601|\n",
      "|   6731|3.5571184995737424|\n",
      "|  48738| 3.895868364160461|\n",
      "|   3210|3.6711219879518073|\n",
      "|  88140|3.5536100302637266|\n",
      "|    467|3.3832658569500675|\n",
      "|   2088| 2.562729584628426|\n",
      "|   2069| 3.806294326241135|\n",
      "|  50802|  2.85519801980198|\n",
      "|    829|2.6765513454146075|\n",
      "|   2136| 2.849462365591398|\n",
      "|  89864|3.8558174523570714|\n",
      "|   2904|3.5884353741496597|\n",
      "|   4821|3.1852010265183917|\n",
      "|  62912|2.3253676470588234|\n",
      "|  55498|2.9166666666666665|\n",
      "|   2162|2.4223394055608822|\n",
      "+-------+------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(avg_rate.limit(20).show())#打印前多少行使用limit来做"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26744\n"
     ]
    }
   ],
   "source": [
    "print(avg_rate.count())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#avg_rate.orderBy(\"rating\").show(20) # \"cannot resolve '`rating`' given input columns: [movieId, avg(rating)]\n",
    "print(avg_rate.orderBy(\"avg(rating)\",descending=True).show())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|movieId|avg(rating)|\n",
      "+-------+-----------+\n",
      "| 131152|        0.5|\n",
      "| 115631|        0.5|\n",
      "| 124368|        0.5|\n",
      "| 117523|        0.5|\n",
      "| 116608|        0.5|\n",
      "| 107106|        0.5|\n",
      "| 120222|        0.5|\n",
      "| 111146|        0.5|\n",
      "| 131062|        0.5|\n",
      "| 105481|        0.5|\n",
      "| 125085|        0.5|\n",
      "| 106866|        0.5|\n",
      "| 109359|        0.5|\n",
      "| 109355|        0.5|\n",
      "|  59775|        0.5|\n",
      "| 121463|        0.5|\n",
      "|  84162|        0.5|\n",
      "| 120763|        0.5|\n",
      "| 111046|        0.5|\n",
      "| 117630|        0.5|\n",
      "+-------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(avg_rate.orderBy(\"avg(rating)\",descending=False).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|movieId|avg(rating)|\n",
      "+-------+-----------+\n",
      "| 113947|        5.0|\n",
      "| 108527|        5.0|\n",
      "|  94431|        5.0|\n",
      "| 113790|        5.0|\n",
      "| 109253|        5.0|\n",
      "| 117606|        5.0|\n",
      "| 105191|        5.0|\n",
      "| 106113|        5.0|\n",
      "|  93707|        5.0|\n",
      "|  95979|        5.0|\n",
      "| 127256|        5.0|\n",
      "| 109715|        5.0|\n",
      "| 106082|        5.0|\n",
      "| 113860|        5.0|\n",
      "|  95517|        5.0|\n",
      "|  27914|        5.0|\n",
      "| 129305|        5.0|\n",
      "|  98761|        5.0|\n",
      "| 111548|        5.0|\n",
      "| 129905|        5.0|\n",
      "+-------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(avg_rate.orderBy([\"avg(rating)\"], ascending=[0]).show()) #这样才能降序排列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min_max_of_movie = rating_data_body.groupby('movieId').agg({\"rating\":[\"min\",\"max\"]}) #java.util.ArrayList cannot be cast to java.lang.String\n",
    "\n",
    "#我想知道每个电影最高评分和最低评分之箭，这样的分组方式要怎么写呢，这个agg函数究竟怎么用呢\n",
    "min_max_of_movie = rating_data_body.groupby('movieId').agg({\"rating\":\"max\",\"rating\":\"min\"}) #参数是一个字典，不过两个键值对只有一个起作用。。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|movieId|min(rating)|\n",
      "+-------+-----------+\n",
      "|    296|        0.5|\n",
      "|   1090|        0.5|\n",
      "|   3959|        0.5|\n",
      "|   2294|        0.5|\n",
      "|   6731|        0.5|\n",
      "|  48738|        0.5|\n",
      "|   3210|        0.5|\n",
      "|  88140|        0.5|\n",
      "|    467|        0.5|\n",
      "|   2088|        0.5|\n",
      "|   2069|        0.5|\n",
      "|  50802|        0.5|\n",
      "|    829|        0.5|\n",
      "|   2136|        0.5|\n",
      "|  89864|        0.5|\n",
      "|   2904|        1.0|\n",
      "|   4821|        0.5|\n",
      "|  62912|        0.5|\n",
      "|  55498|        0.5|\n",
      "|   2162|        0.5|\n",
      "+-------+-----------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(min_max_of_movie.limit(20).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_of_movie = rating_data_body.groupby('movieId').agg({\"rating\":\"max,min\"})  \n",
    "#只能是定义好的一个函数，不能是多个函数写一起，不过源码里面说可以是一个列表Undefined function: 'max,min'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_of_movie = rating_data_body.groupby('movieId').agg({\"rating\":[\"max\",\"min\"]})  #ArrayList cannot be cast to java.lang.String\n",
    "#折腾这么写东西说实话还是直接转成sql查询就好，更加简单方便，spark sql 可以用来做大数据量的统计分析，小数据量的使用pandas来做，这样就形成了互补\n",
    "#spark sql就暂时先了解到这吧，实际运用中遇到什么问题再搜索就行了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
